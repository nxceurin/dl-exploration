{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-LuEzWcGQ38_"
      },
      "source": [
        "Implementation of Adam and RMSProp optimizers from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "grrYyrAMQqQb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "28VQ-Sr5Qnu8"
      },
      "outputs": [],
      "source": [
        "class AdamOptim():\n",
        "    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "      self.m_dw, self.v_dw = 0, 0\n",
        "      self.m_db, self.v_db = 0, 0\n",
        "      self.beta1 = beta1\n",
        "      self.beta2 = beta2\n",
        "      self.epsilon = epsilon\n",
        "      self.eta = eta\n",
        "\n",
        "    def update(self, t, w, b, dw, db):\n",
        "      #momentum grad\n",
        "      self.m_dw = self.beta1*self.m_dw + ((1-self.beta1)*dw)\n",
        "      self.m_db = self.beta1*self.m_db + ((1-self.beta1)*db)\n",
        "      \n",
        "      #rmsprop\n",
        "      self.v_dw = self.beta2*self.v_dw + ((1-self.beta2)*(dw**2))\n",
        "      self.v_db = self.beta2*self.v_db + ((1-self.beta2)*(db**2))\n",
        "      \n",
        "      ## bias correction\n",
        "      m_dw_corr = self.m_dw/(1-self.beta1)\n",
        "      m_db_corr = self.m_db/(1-self.beta1)\n",
        "      v_dw_corr = self.v_dw/(1-self.beta2)\n",
        "      v_db_corr = self.v_db/(1-self.beta2)\n",
        "\n",
        "      ## update weights and biases\n",
        "      w = w- self.eta* (m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
        "      b = b- self.eta* (m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))\n",
        "      loss= loss_function(w)\n",
        "      return w, b, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w0saAsWJRcF0"
      },
      "outputs": [],
      "source": [
        "def loss_function(m):\n",
        "    return m**2-2*m+1\n",
        "\n",
        "## take derivative\n",
        "def grad_function(m):\n",
        "    return 2*m-2\n",
        "\n",
        "def check_convergence(w0, w1):\n",
        "    return (w0 == w1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYk2OdjNRdfR",
        "outputId": "26f7277f-f127-4f59-839f-d85b8775aa33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration 1: weight=0.36726408604516814 loss:0.4003547368082564\n",
            "iteration 2: weight=0.3806965694055127 loss:0.38353673914610087\n",
            "iteration 3: weight=0.39632856383774606 loss:0.3644192028381983\n",
            "iteration 4: weight=0.4134847200997135 loss:0.34400017355651147\n",
            "iteration 5: weight=0.4317193260881454 loss:0.3229429243417117\n",
            "iteration 6: weight=0.4507108745984828 loss:0.30171854328436365\n",
            "iteration 7: weight=0.47021514390803837 loss:0.28067199374438045\n",
            "iteration 8: weight=0.4900403324950345 loss:0.260058862481775\n",
            "iteration 9: weight=0.5100323300499237 loss:0.24006831759630698\n",
            "iteration 10: weight=0.53006529043319 loss:0.22083863125564207\n",
            "iteration 11: weight=0.5500352449275143 loss:0.20246828080744206\n",
            "iteration 12: weight=0.5698555833419586 loss:0.18502421918208678\n",
            "iteration 13: weight=0.5894537459160937 loss:0.1685482267423274\n",
            "iteration 14: weight=0.6087687344784813 loss:0.15306190312156898\n",
            "iteration 15: weight=0.62774919762135 loss:0.13857065987154882\n",
            "iteration 16: weight=0.6463519298205471 loss:0.1250669575416512\n",
            "iteration 17: weight=0.6645406763198903 loss:0.11253295784391659\n",
            "iteration 18: weight=0.6822851684583483 loss:0.1009427141815401\n",
            "iteration 19: weight=0.6995603356285816 loss:0.09026399192761059\n",
            "iteration 20: weight=0.7163456545494091 loss:0.0804597876930031\n",
            "iteration 21: weight=0.7326246065541607 loss:0.07148960102031732\n",
            "iteration 22: weight=0.7483842206835722 loss:0.06331050040101327\n",
            "iteration 23: weight=0.7636146854896458 loss:0.05587801691615901\n",
            "iteration 24: weight=0.7783090162240968 loss:0.04914689228752778\n",
            "iteration 25: weight=0.7924627669077104 loss:0.04307170311960329\n",
            "iteration 26: weight=0.8060737789272638 loss:0.037607379219551706\n",
            "iteration 27: weight=0.8191419594703452 loss:0.03270963082422629\n",
            "iteration 28: weight=0.8316690844141755 loss:0.028335297141961924\n",
            "iteration 29: weight=0.8436586213229127 loss:0.024442626686652424\n",
            "iteration 30: weight=0.8551155690433284 loss:0.02099149833363856\n",
            "iteration 31: weight=0.866046311067553 loss:0.01794359077861074\n",
            "iteration 32: weight=0.8764584803875215 loss:0.015262507068160458\n",
            "iteration 33: weight=0.8863608340236271 loss:0.01291386004380568\n",
            "iteration 34: weight=0.8957631357883622 loss:0.010865323860675402\n",
            "iteration 35: weight=0.9046760461583387 loss:0.009086656176007124\n",
            "iteration 36: weight=0.9131110183857384 loss:0.007549695125963485\n",
            "iteration 37: weight=0.9210802001913534 loss:0.006228334801836821\n",
            "iteration 38: weight=0.9285963405527012 loss:0.005098482582465813\n",
            "iteration 39: weight=0.935672701237812 loss:0.004138001366039812\n",
            "iteration 40: weight=0.9423229728429484 loss:0.003326639461675218\n",
            "iteration 41: weight=0.948561195174853 loss:0.0026459506418395895\n",
            "iteration 42: weight=0.9544016818787945 loss:0.002079206615482665\n",
            "iteration 43: weight=0.9598589492560187 loss:0.0016113039548308894\n",
            "iteration 44: weight=0.9649476492412061 loss:0.0012286672937175114\n",
            "iteration 45: weight=0.9696825065249287 loss:0.0009191504106109694\n",
            "iteration 46: weight=0.9740782598103759 loss:0.0006719366144584127\n",
            "iteration 47: weight=0.9781496071900158 loss:0.0004774396659505964\n",
            "iteration 48: weight=0.9819111556183633 loss:0.00032720629106308685\n",
            "iteration 49: weight=0.9853773744433939 loss:0.00021382117816870494\n",
            "iteration 50: weight=0.9885625529428755 loss:0.00013081519518454154\n",
            "iteration 51: weight=0.9914807617942661 loss:7.257741960609021e-05\n",
            "iteration 52: weight=0.9941458183888923 loss:3.4271442335809255e-05\n",
            "iteration 53: weight=0.9965712558837215 loss:1.1756286214859024e-05\n",
            "iteration 54: weight=0.9987702958678185 loss:1.5121722527489112e-06\n",
            "iteration 55: weight=1.000755824506003 loss:5.712706838689741e-07\n",
            "iteration 56: weight=1.002540372009628 loss:6.453489947322311e-06\n",
            "iteration 57: weight=1.0041360952739549 loss:1.7107284115303045e-05\n",
            "iteration 58: weight=1.0055547635134061 loss:3.0855397689766306e-05\n",
            "iteration 59: weight=1.0068077467199994 loss:4.6345415403603596e-05\n",
            "iteration 60: weight=1.0079060067664483 loss:6.25049429912039e-05\n",
            "iteration 61: weight=1.0088600909736072 loss:7.850121206054439e-05\n",
            "iteration 62: weight=1.009680127961977 loss:9.370487736015676e-05\n",
            "iteration 63: weight=1.01037582560868 loss:0.00010765775706178538\n",
            "iteration 64: weight=1.0109564709344545 loss:0.00012004425533751473\n",
            "iteration 65: weight=1.0114309317495904 loss:0.00013066620066370938\n",
            "iteration 66: weight=1.0118076598931411 loss:0.0001394208321521262\n",
            "iteration 67: weight=1.0120946959059884 loss:0.0001462816690582791\n",
            "iteration 68: weight=1.0122996749852324 loss:0.000151282004742459\n",
            "iteration 69: weight=1.0124298340747613 loss:0.00015450077512602078\n",
            "iteration 70: weight=1.0124920199545726 loss:0.00015605056254552885\n",
            "iteration 71: weight=1.0124926981993316 loss:0.0001560675082996621\n",
            "iteration 72: weight=1.0124379628846618 loss:0.00015470292072028435\n",
            "iteration 73: weight=1.0123335469276526 loss:0.00015211637981660253\n",
            "iteration 74: weight=1.012184832955971 loss:0.00014847015416497022\n",
            "iteration 75: weight=1.0119968646077078 loss:0.00014392476041558133\n",
            "iteration 76: weight=1.0117743581716077 loss:0.0001386355103532022\n",
            "iteration 77: weight=1.0115217144846154 loss:0.00013274990466505265\n",
            "iteration 78: weight=1.0112430310106395 loss:0.00012640574630617962\n",
            "iteration 79: weight=1.0109421140311208 loss:0.00011972985946995252\n",
            "iteration 80: weight=1.0106224908843344 loss:0.00011283731258782126\n",
            "iteration 81: weight=1.0102874221963731 loss:0.00010583105544648141\n",
            "iteration 82: weight=1.0099399140524365 loss:9.880189136990403e-05\n",
            "iteration 83: weight=1.0095827300624005 loss:9.18287154487718e-05\n",
            "iteration 84: weight=1.0092184032796496 loss:8.497895902626063e-05\n",
            "iteration 85: weight=1.0088492479368532 loss:7.83091890479426e-05\n",
            "iteration 86: weight=1.0084773709667394 loss:7.186581850771034e-05\n",
            "iteration 87: weight=1.0081046832800025 loss:6.568589106925593e-05\n",
            "iteration 88: weight=1.0077329107762596 loss:5.9797909073511235e-05\n",
            "iteration 89: weight=1.0073636050674872 loss:5.422267958987703e-05\n",
            "iteration 90: weight=1.0069981538966035 loss:4.8974157960568476e-05\n",
            "iteration 91: weight=1.0066377912368611 loss:4.406027250425382e-05\n",
            "iteration 92: weight=1.0062836070604637 loss:3.948371769024739e-05\n",
            "iteration 93: weight=1.0059365567673464 loss:3.524270625199222e-05\n",
            "iteration 94: weight=1.0055974702673771 loss:3.133167339419707e-05\n",
            "iteration 95: weight=1.0052670607113436 loss:2.7741928537050597e-05\n",
            "iteration 96: weight=1.0049459328680195 loss:2.4462251934975754e-05\n",
            "iteration 97: weight=1.0046345911463421 loss:2.1479435093674226e-05\n",
            "iteration 98: weight=1.0043334472633176 loss:1.8778765183968815e-05\n",
            "iteration 99: weight=1.0040428275596882 loss:1.6344454677463816e-05\n",
            "iteration 100: weight=1.003762979966667 loss:1.4160018229514293e-05\n",
            "iteration 101: weight=1.0034940806281896 loss:1.2208599436291934e-05\n",
            "iteration 102: weight=1.0032362401841335 loss:1.047325052949688e-05\n",
            "iteration 103: weight=1.0029895097208454 loss:8.937168371137005e-06\n",
            "iteration 104: weight=1.0027538863960965 loss:7.583890282658601e-06\n",
            "iteration 105: weight=1.0025293187462525 loss:6.397453320206026e-06\n",
            "iteration 106: weight=1.002315711684025 loss:5.36252060356901e-06\n",
            "iteration 107: weight=1.0021129311956543 loss:4.46447823754248e-06\n",
            "iteration 108: weight=1.001920808746777 loss:3.6895062416331115e-06\n",
            "iteration 109: weight=1.0017391454065572 loss:3.024626745062875e-06\n",
            "iteration 110: weight=1.001567715699917 loss:2.4577325157260077e-06\n",
            "iteration 111: weight=1.001406271197891 loss:1.977598681923709e-06\n",
            "iteration 112: weight=1.0012545438562603 loss:1.5738802872089508e-06\n",
            "iteration 113: weight=1.0011122491126994 loss:1.237098088635591e-06\n",
            "iteration 114: weight=1.0009790887526933 loss:9.586147855511484e-07\n",
            "iteration 115: weight=1.0008547535544685 loss:7.30603638920968e-07\n",
            "iteration 116: weight=1.0007389257231183 loss:5.460112242339221e-07\n",
            "iteration 117: weight=1.0006312811240088 loss:3.9851585742489704e-07\n",
            "iteration 118: weight=1.000531491325427 loss:2.8248302896827227e-07\n",
            "iteration 119: weight=1.00043922546027 loss:1.9291900499318615e-07\n",
            "iteration 120: weight=1.0003541519163965 loss:1.254235799663661e-07\n",
            "iteration 121: weight=1.0002759398650523 loss:7.614280916889982e-08\n",
            "iteration 122: weight=1.0002042606365602 loss:4.1722407750910406e-08\n",
            "iteration 123: weight=1.0001387889522215 loss:1.926237325200475e-08\n",
            "iteration 124: weight=1.0000792040211186 loss:6.2732770089724e-09\n",
            "iteration 125: weight=1.0000251905102444 loss:6.345617364900136e-10\n",
            "iteration 126: weight=0.9999764393961064 loss:5.55102075416869e-10\n",
            "iteration 127: weight=0.9999326487056643 loss:4.536196862581221e-09\n",
            "iteration 128: weight=0.999893524154177 loss:1.133710569867219e-08\n",
            "iteration 129: weight=0.9998587796872338 loss:1.9943176776671123e-08\n",
            "iteration 130: weight=0.9998281379339546 loss:2.953656974558072e-08\n",
            "iteration 131: weight=0.9998013305780405 loss:3.946953919253815e-08\n",
            "iteration 132: weight=0.9997780986530649 loss:4.924020780006089e-08\n",
            "iteration 133: weight=0.9997581927680964 loss:5.847073736298114e-08\n",
            "iteration 134: weight=0.9997413732694552 loss:6.688778575458798e-08\n",
            "iteration 135: weight=0.9997274103441132 loss:7.43051205009948e-08\n",
            "iteration 136: weight=0.999716084069966 loss:8.060825529287996e-08\n",
            "iteration 137: weight=0.9997071844179272 loss:8.574096510560736e-08\n",
            "iteration 138: weight=0.999700511210517 loss:8.969353504362232e-08\n",
            "iteration 139: weight=0.9996958740413593 loss:9.249259869115178e-08\n",
            "iteration 140: weight=0.9996930921597336 loss:9.41924224173718e-08\n",
            "iteration 141: weight=0.9996919943240822 loss:9.486749641407499e-08\n",
            "iteration 142: weight=0.9996924186281252 loss:9.460630034929807e-08\n",
            "iteration 143: weight=0.9996942123030021 loss:9.350611562819466e-08\n",
            "iteration 144: weight=0.9996972314986295 loss:9.166876546728986e-08\n",
            "iteration 145: weight=0.9997013410472448 loss:8.919717009447936e-08\n",
            "iteration 146: weight=0.9997064142118961 loss:8.619261493425512e-08\n",
            "iteration 147: weight=0.9997123324224345 loss:8.27526351887542e-08\n",
            "iteration 148: weight=0.9997189850013734 loss:7.89694294400789e-08\n",
            "iteration 149: weight=0.9997262688817914 loss:7.492872511338788e-08\n",
            "iteration 150: weight=0.9997340883192819 loss:7.070902197092721e-08\n",
            "iteration 151: weight=0.9997423545997817 loss:6.638115224166796e-08\n",
            "iteration 152: weight=0.9997509857449536 loss:6.200809921086403e-08\n",
            "iteration 153: weight=0.999759906216645 loss:5.764502475358313e-08\n",
            "iteration 154: weight=0.9997690466218017 loss:5.333946284657998e-08\n",
            "iteration 155: weight=0.9997783434190798 loss:4.913163986763891e-08\n",
            "iteration 156: weight=0.9997877386282737 loss:4.505488993000739e-08\n",
            "iteration 157: weight=0.999797179543554 loss:4.113613760736712e-08\n",
            "iteration 158: weight=0.9998066184513991 loss:3.7396423291369274e-08\n",
            "iteration 159: weight=0.9998160123539988 loss:3.3851453862254743e-08\n",
            "iteration 160: weight=0.9998253226988065 loss:3.0512159576723263e-08\n",
            "iteration 161: weight=0.9998345151148279 loss:2.7385247181044292e-08\n",
            "iteration 162: weight=0.9998435591561449 loss:2.4473737592067835e-08\n",
            "iteration 163: weight=0.9998524280530967 loss:2.1777479486395634e-08\n",
            "iteration 164: weight=0.9998610984714641 loss:1.9293634578865237e-08\n",
            "iteration 165: weight=0.9998695502799347 loss:1.7017129483321014e-08\n",
            "iteration 166: weight=0.9998777663260665 loss:1.494107104704767e-08\n",
            "iteration 167: weight=0.9998857322209049 loss:1.3057125380910861e-08\n",
            "iteration 168: weight=0.999893436132361 loss:1.1355857920669621e-08\n",
            "iteration 169: weight=0.9999008685874055 loss:9.827036961951308e-09\n",
            "iteration 170: weight=0.9999080222830925 loss:8.459900446844415e-09\n",
            "iteration 171: weight=0.9999148919063862 loss:7.24338755642151e-09\n",
            "iteration 172: weight=0.9999214739627271 loss:6.1663385508836654e-09\n",
            "iteration 173: weight=0.9999277666132449 loss:5.2176621911925736e-09\n",
            "iteration 174: weight=0.999933769520493 loss:4.386476404327766e-09\n",
            "iteration 175: weight=0.999939483702558 loss:3.6622223031912426e-09\n",
            "iteration 176: weight=0.9999449113953734 loss:3.0347543367170715e-09\n",
            "iteration 177: weight=0.9999500559230466 loss:2.494410789033452e-09\n",
            "iteration 178: weight=0.9999549215759941 loss:2.0320642946103362e-09\n",
            "iteration 179: weight=0.9999595134966638 loss:1.6391569213070056e-09\n",
            "iteration 180: weight=0.9999638375726122 loss:1.307721153587238e-09\n",
            "iteration 181: weight=0.9999679003366945 loss:1.0303883302142935e-09\n",
            "iteration 182: weight=0.9999717088741167 loss:8.003877560724959e-10\n",
            "iteration 183: weight=0.9999752707360967 loss:6.115364881154051e-10\n",
            "iteration 184: weight=0.9999785938598714 loss:4.582227930427507e-10\n",
            "iteration 185: weight=0.9999816864947896 loss:3.35384497951452e-10\n",
            "iteration 186: weight=0.9999845571342256 loss:2.384821229384215e-10\n",
            "iteration 187: weight=0.9999872144530509 loss:1.634702373465302e-10\n",
            "iteration 188: weight=0.9999896672503995 loss:1.067657073861028e-10\n",
            "iteration 189: weight=0.9999919243974682 loss:6.52153886449014e-11\n",
            "iteration 190: weight=0.9999939947900932 loss:3.606259735278172e-11\n",
            "iteration 191: weight=0.99999588730585 loss:1.691424778016426e-11\n",
            "iteration 192: weight=0.9999976107654273 loss:5.708433725715167e-12\n",
            "iteration 193: weight=0.999999173898032 loss:6.824540932370837e-13\n",
            "iteration 194: weight=1.0000005853105864 loss:3.426148253993233e-13\n",
            "iteration 195: weight=1.0000018534604878 loss:3.4352520827951594e-12\n",
            "iteration 196: weight=1.0000029866317026 loss:8.919975869048358e-12\n",
            "iteration 197: weight=1.0000039929139815 loss:1.5943468767432023e-11\n",
            "iteration 198: weight=1.0000048801849812 loss:2.3816282279653933e-11\n",
            "iteration 199: weight=1.0000056560950925 loss:3.1991520543783736e-11\n",
            "iteration 200: weight=1.0000063280547777 loss:4.004419018599492e-11\n",
            "iteration 201: weight=1.000006903224231 loss:4.7654546975195444e-11\n",
            "iteration 202: weight=1.0000073885051817 loss:5.45901102100288e-11\n",
            "iteration 203: weight=1.0000077905346696 loss:6.069234004257851e-11\n",
            "iteration 204: weight=1.0000081156806258 loss:6.586420298049234e-11\n",
            "iteration 205: weight=1.0000083700391076 loss:7.005751534450155e-11\n",
            "iteration 206: weight=1.0000085594330348 loss:7.3263839439619e-11\n",
            "iteration 207: weight=1.0000086894122895 loss:7.550582381554705e-11\n",
            "iteration 208: weight=1.0000087652550462 loss:7.682965375011008e-11\n",
            "iteration 209: weight=1.0000087919702054 loss:7.729883400031667e-11\n",
            "iteration 210: weight=1.000008774300813 loss:7.698841564263148e-11\n",
            "iteration 211: weight=1.0000087167283558 loss:7.598144335929646e-11\n",
            "iteration 212: weight=1.0000086234778254 loss:7.436429250162746e-11\n",
            "iteration 213: weight=1.0000084985234563 loss:7.222489273317478e-11\n",
            "iteration 214: weight=1.0000083455950441 loss:6.96489532714395e-11\n",
            "iteration 215: weight=1.0000081681847612 loss:6.671929675405863e-11\n",
            "iteration 216: weight=1.000007969554389 loss:6.351386083736088e-11\n",
            "iteration 217: weight=1.0000077527428952 loss:6.01050320625518e-11\n",
            "iteration 218: weight=1.0000075205742858 loss:5.655897972189905e-11\n",
            "iteration 219: weight=1.0000072756656735 loss:5.293521176952254e-11\n",
            "iteration 220: weight=1.0000070204355016 loss:4.9286574821394424e-11\n",
            "iteration 221: weight=1.000006757111873 loss:4.565858802152434e-11\n",
            "iteration 222: weight=1.0000064877409347 loss:4.2090775309588935e-11\n",
            "iteration 223: weight=1.0000062141952772 loss:3.8616221331722045e-11\n",
            "iteration 224: weight=1.0000059381823065 loss:3.526201552972452e-11\n",
            "iteration 225: weight=1.0000056612525552 loss:3.2049696230274094e-11\n",
            "iteration 226: weight=1.0000053848078998 loss:2.8996138823345063e-11\n",
            "iteration 227: weight=1.0000051101096574 loss:2.6113111672998457e-11\n",
            "iteration 228: weight=1.0000048382865359 loss:2.3409052474221426e-11\n",
            "iteration 229: weight=1.0000045703424152 loss:2.088795802990262e-11\n",
            "iteration 230: weight=1.0000043071639428 loss:1.855160469688144e-11\n",
            "iteration 231: weight=1.0000040495279243 loss:1.6398660207528337e-11\n",
            "iteration 232: weight=1.0000037981084977 loss:1.4425571848164509e-11\n",
            "iteration 233: weight=1.0000035534840785 loss:1.262723259287668e-11\n",
            "iteration 234: weight=1.000003316144067 loss:1.0996759058912176e-11\n",
            "iteration 235: weight=1.0000030864953093 loss:9.526379685098618e-12\n",
            "iteration 236: weight=1.0000028648683086 loss:8.207434731843932e-12\n",
            "iteration 237: weight=1.0000026515231797 loss:7.030598325741266e-12\n",
            "iteration 238: weight=1.0000024466553479 loss:5.986100504173919e-12\n",
            "iteration 239: weight=1.0000022504009882 loss:5.064393349130114e-12\n",
            "iteration 240: weight=1.0000020628422077 loss:4.2552628087833e-12\n",
            "iteration 241: weight=1.0000018840119702 loss:3.5496050543315505e-12\n",
            "iteration 242: weight=1.0000017138987676 loss:2.937428078553239e-12\n",
            "iteration 243: weight=1.00000155245104 loss:2.41007214185629e-12\n",
            "iteration 244: weight=1.0000013995813488 loss:1.958877504648626e-12\n",
            "iteration 245: weight=1.0000012551703092 loss:1.5754064719430971e-12\n",
            "iteration 246: weight=1.0000011190702855 loss:1.2523315717771766e-12\n",
            "iteration 247: weight=1.0000009911088563 loss:9.823253321883385e-13\n",
            "iteration 248: weight=1.0000008710920558 loss:7.58726415028832e-13\n",
            "iteration 249: weight=1.0000007588073991 loss:5.757616605706062e-13\n",
            "iteration 250: weight=1.0000006540266975 loss:4.276579090856103e-13\n",
            "iteration 251: weight=1.0000005565086705 loss:3.097522238704187e-13\n",
            "iteration 252: weight=1.000000466001365 loss:2.1715962361668062e-13\n",
            "iteration 253: weight=1.0000003822443875 loss:1.461053500406706e-13\n",
            "iteration 254: weight=1.0000003049709554 loss:9.303668946358812e-14\n",
            "iteration 255: weight=1.0000002339097807 loss:5.46229728115577e-14\n",
            "iteration 256: weight=1.0000001687867883 loss:2.842170943040401e-14\n",
            "iteration 257: weight=1.0000001093266802 loss:1.199040866595169e-14\n",
            "iteration 258: weight=1.0000000552543522 loss:3.1086244689504383e-15\n",
            "iteration 259: weight=1.000000006296172 loss:0.0\n",
            "iteration 260: weight=0.9999999621811249 loss:1.4432899320127035e-15\n",
            "iteration 261: weight=0.9999999226418351 loss:5.995204332975845e-15\n",
            "iteration 262: weight=0.9999998874154713 loss:1.2656542480726785e-14\n",
            "iteration 263: weight=0.999999856244541 loss:2.0650148258027912e-14\n",
            "iteration 264: weight=0.999999828877584 loss:2.930988785010413e-14\n",
            "iteration 265: weight=0.9999998050697683 loss:3.7969627442180354e-14\n",
            "iteration 266: weight=0.9999997845833977 loss:4.6407322429331543e-14\n",
            "iteration 267: weight=0.9999997671883363 loss:5.417888360170764e-14\n",
            "iteration 268: weight=0.9999997526623556 loss:6.117328865684613e-14\n",
            "iteration 269: weight=0.9999997407914115 loss:6.716849298982197e-14\n",
            "iteration 270: weight=0.9999997313698549 loss:7.216449660063518e-14\n",
            "iteration 271: weight=0.9999997242005827 loss:7.605027718682322e-14\n",
            "iteration 272: weight=0.9999997190951343 loss:7.893685705084863e-14\n",
            "iteration 273: weight=0.9999997158737376 loss:8.071321389024888e-14\n",
            "iteration 274: weight=0.9999997143653099 loss:8.1601392309949e-14\n",
            "iteration 275: weight=0.9999997144074175 loss:8.1601392309949e-14\n",
            "iteration 276: weight=0.9999997158461995 loss:8.071321389024888e-14\n",
            "iteration 277: weight=0.9999997185362575 loss:7.926992395823618e-14\n",
            "iteration 278: weight=0.9999997223405167 loss:7.704947790898586e-14\n",
            "iteration 279: weight=0.999999727130062 loss:7.4495964952348e-14\n",
            "iteration 280: weight=0.9999997327839507 loss:7.138734048339757e-14\n",
            "iteration 281: weight=0.9999997391890063 loss:6.80566714095221e-14\n",
            "iteration 282: weight=0.9999997462395959 loss:6.439293542825908e-14\n",
            "iteration 283: weight=0.9999997538373935 loss:6.061817714453355e-14\n",
            "iteration 284: weight=0.9999997618911322 loss:5.67323965583455e-14\n",
            "iteration 285: weight=0.9999997703163469 loss:5.2735593669694936e-14\n",
            "iteration 286: weight=0.9999997790351105 loss:4.884981308350689e-14\n",
            "iteration 287: weight=0.9999997879757643 loss:4.496403249731884e-14\n",
            "iteration 288: weight=0.9999997970726454 loss:4.118927421359331e-14\n",
            "iteration 289: weight=0.9999998062658126 loss:3.752553823233029e-14\n",
            "iteration 290: weight=0.9999998155007715 loss:3.4083846855992306e-14\n",
            "iteration 291: weight=0.9999998247282008 loss:3.0753177782116836e-14\n",
            "iteration 292: weight=0.9999998339036809 loss:2.7533531010703882e-14\n",
            "iteration 293: weight=0.9999998429874256 loss:2.4646951146678475e-14\n",
            "iteration 294: weight=0.9999998519440186 loss:2.1871393585115584e-14\n",
            "iteration 295: weight=0.9999998607421539 loss:1.942890293094024e-14\n",
            "iteration 296: weight=0.9999998693543828 loss:1.709743457922741e-14\n",
            "iteration 297: weight=0.9999998777568674 loss:1.4988010832439613e-14\n",
            "iteration 298: weight=0.9999998859291404 loss:1.2989609388114332e-14\n",
            "iteration 299: weight=0.9999998938538727 loss:1.1213252548714081e-14\n",
            "iteration 300: weight=0.9999999015166489 loss:9.658940314238862e-15\n",
            "iteration 301: weight=0.9999999089057504 loss:8.326672684688674e-15\n",
            "iteration 302: weight=0.9999999160119473 loss:7.105427357601002e-15\n",
            "iteration 303: weight=0.9999999228282984 loss:5.995204332975845e-15\n",
            "iteration 304: weight=0.9999999293499601 loss:4.9960036108132044e-15\n",
            "iteration 305: weight=0.9999999355740036 loss:4.107825191113079e-15\n",
            "iteration 306: weight=0.9999999414992413 loss:3.4416913763379853e-15\n",
            "iteration 307: weight=0.9999999471260617 loss:2.7755575615628914e-15\n",
            "iteration 308: weight=0.9999999524562722 loss:2.220446049250313e-15\n",
            "iteration 309: weight=0.9999999574929516 loss:1.7763568394002505e-15\n",
            "iteration 310: weight=0.9999999622403101 loss:1.4432899320127035e-15\n",
            "iteration 311: weight=0.999999966703558 loss:1.1102230246251565e-15\n",
            "iteration 312: weight=0.999999970888782 loss:8.881784197001252e-16\n",
            "iteration 313: weight=0.9999999748028301 loss:6.661338147750939e-16\n",
            "iteration 314: weight=0.9999999784532031 loss:4.440892098500626e-16\n",
            "iteration 315: weight=0.9999999818479546 loss:3.3306690738754696e-16\n",
            "iteration 316: weight=0.9999999849955973 loss:2.220446049250313e-16\n",
            "iteration 317: weight=0.9999999879050168 loss:1.1102230246251565e-16\n",
            "iteration 318: weight=0.9999999905853916 loss:1.1102230246251565e-16\n",
            "iteration 319: weight=0.9999999930461199 loss:0.0\n",
            "iteration 320: weight=0.9999999952967522 loss:0.0\n",
            "iteration 321: weight=0.9999999973469301 loss:0.0\n",
            "iteration 322: weight=0.9999999992063301 loss:0.0\n",
            "iteration 323: weight=1.000000000884613 loss:0.0\n",
            "iteration 324: weight=1.0000000023913789 loss:0.0\n",
            "iteration 325: weight=1.0000000037361256 loss:0.0\n",
            "iteration 326: weight=1.000000004928213 loss:0.0\n",
            "iteration 327: weight=1.0000000059768304 loss:0.0\n",
            "iteration 328: weight=1.0000000068909685 loss:0.0\n",
            "iteration 329: weight=1.0000000076793953 loss:0.0\n",
            "iteration 330: weight=1.0000000083506346 loss:0.0\n",
            "iteration 331: weight=1.0000000089129488 loss:0.0\n",
            "iteration 332: weight=1.0000000093743238 loss:0.0\n",
            "iteration 333: weight=1.0000000097424566 loss:0.0\n",
            "iteration 334: weight=1.0000000100247473 loss:0.0\n",
            "iteration 335: weight=1.0000000102282904 loss:0.0\n",
            "iteration 336: weight=1.0000000103598712 loss:0.0\n",
            "iteration 337: weight=1.000000010425962 loss:0.0\n",
            "iteration 338: weight=1.000000010432722 loss:0.0\n",
            "iteration 339: weight=1.000000010385997 loss:0.0\n",
            "iteration 340: weight=1.0000000102913214 loss:0.0\n",
            "iteration 341: weight=1.000000010153922 loss:0.0\n",
            "iteration 342: weight=1.0000000099787225 loss:0.0\n",
            "iteration 343: weight=1.000000009770349 loss:0.0\n",
            "iteration 344: weight=1.0000000095331358 loss:0.0\n",
            "iteration 345: weight=1.0000000092711339 loss:0.0\n",
            "iteration 346: weight=1.0000000089881174 loss:0.0\n",
            "iteration 347: weight=1.0000000086875933 loss:0.0\n",
            "iteration 348: weight=1.0000000083728098 loss:0.0\n",
            "iteration 349: weight=1.0000000080467657 loss:0.0\n",
            "iteration 350: weight=1.0000000077122202 loss:0.0\n",
            "iteration 351: weight=1.000000007371703 loss:0.0\n",
            "iteration 352: weight=1.000000007027524 loss:0.0\n",
            "iteration 353: weight=1.0000000066817831 loss:0.0\n",
            "iteration 354: weight=1.0000000063363816 loss:0.0\n",
            "iteration 355: weight=1.0000000059930312 loss:0.0\n",
            "iteration 356: weight=1.0000000056532643 loss:0.0\n",
            "iteration 357: weight=1.0000000053184444 loss:0.0\n",
            "iteration 358: weight=1.0000000049897757 loss:0.0\n",
            "iteration 359: weight=1.0000000046683128 loss:0.0\n",
            "iteration 360: weight=1.0000000043549702 loss:0.0\n",
            "iteration 361: weight=1.000000004050531 loss:0.0\n",
            "iteration 362: weight=1.000000003755657 loss:0.0\n",
            "iteration 363: weight=1.000000003470896 loss:0.0\n",
            "iteration 364: weight=1.0000000031966916 loss:0.0\n",
            "iteration 365: weight=1.00000000293339 loss:0.0\n",
            "iteration 366: weight=1.0000000026812488 loss:0.0\n",
            "iteration 367: weight=1.0000000024404438 loss:0.0\n",
            "iteration 368: weight=1.0000000022110764 loss:0.0\n",
            "iteration 369: weight=1.0000000019931803 loss:0.0\n",
            "iteration 370: weight=1.000000001786728 loss:0.0\n",
            "iteration 371: weight=1.0000000015916375 loss:0.0\n",
            "iteration 372: weight=1.0000000014077768 loss:0.0\n",
            "iteration 373: weight=1.0000000012349708 loss:0.0\n",
            "iteration 374: weight=1.0000000010730055 loss:0.0\n",
            "iteration 375: weight=1.0000000009216334 loss:0.0\n",
            "iteration 376: weight=1.0000000007805778 loss:0.0\n",
            "iteration 377: weight=1.0000000006495369 loss:0.0\n",
            "iteration 378: weight=1.0000000005281882 loss:0.0\n",
            "iteration 379: weight=1.000000000416192 loss:0.0\n",
            "iteration 380: weight=1.0000000003131944 loss:0.0\n",
            "iteration 381: weight=1.000000000218831 loss:0.0\n",
            "iteration 382: weight=1.0000000001327296 loss:0.0\n",
            "iteration 383: weight=1.0000000000545128 loss:0.0\n",
            "iteration 384: weight=0.9999999999838003 loss:0.0\n",
            "iteration 385: weight=0.999999999920211 loss:0.0\n",
            "iteration 386: weight=0.9999999998633656 loss:0.0\n",
            "iteration 387: weight=0.9999999998128876 loss:0.0\n",
            "iteration 388: weight=0.9999999997684053 loss:0.0\n",
            "iteration 389: weight=0.9999999997295532 loss:0.0\n",
            "iteration 390: weight=0.9999999996959733 loss:0.0\n",
            "iteration 391: weight=0.9999999996673159 loss:0.0\n",
            "iteration 392: weight=0.9999999996432407 loss:0.0\n",
            "iteration 393: weight=0.9999999996234176 loss:0.0\n",
            "iteration 394: weight=0.9999999996075274 loss:0.0\n",
            "iteration 395: weight=0.9999999995952622 loss:0.0\n",
            "iteration 396: weight=0.9999999995863261 loss:0.0\n",
            "iteration 397: weight=0.9999999995804354 loss:0.0\n",
            "iteration 398: weight=0.9999999995773187 loss:0.0\n",
            "iteration 399: weight=0.999999999576717 loss:0.0\n",
            "iteration 400: weight=0.9999999995783845 loss:0.0\n",
            "iteration 401: weight=0.9999999995820874 loss:0.0\n",
            "iteration 402: weight=0.999999999587605 loss:0.0\n",
            "iteration 403: weight=0.9999999995947289 loss:0.0\n",
            "iteration 404: weight=0.9999999996032629 loss:0.0\n",
            "iteration 405: weight=0.9999999996130231 loss:0.0\n",
            "iteration 406: weight=0.9999999996238375 loss:0.0\n",
            "iteration 407: weight=0.9999999996355453 loss:0.0\n",
            "iteration 408: weight=0.9999999996479974 loss:0.0\n",
            "iteration 409: weight=0.9999999996610552 loss:0.0\n",
            "iteration 410: weight=0.9999999996745911 loss:0.0\n",
            "iteration 411: weight=0.9999999996884871 loss:0.0\n",
            "iteration 412: weight=0.9999999997026353 loss:0.0\n",
            "iteration 413: weight=0.9999999997169372 loss:0.0\n",
            "iteration 414: weight=0.999999999731303 loss:0.0\n",
            "iteration 415: weight=0.9999999997456517 loss:0.0\n",
            "iteration 416: weight=0.99999999975991 loss:0.0\n",
            "iteration 417: weight=0.9999999997740127 loss:0.0\n",
            "iteration 418: weight=0.9999999997879014 loss:0.0\n",
            "iteration 419: weight=0.9999999998015252 loss:0.0\n",
            "iteration 420: weight=0.999999999814839 loss:0.0\n",
            "iteration 421: weight=0.999999999827804 loss:0.0\n",
            "iteration 422: weight=0.9999999998403869 loss:0.0\n",
            "iteration 423: weight=0.9999999998525598 loss:0.0\n",
            "iteration 424: weight=0.9999999998642998 loss:0.0\n",
            "iteration 425: weight=0.9999999998755882 loss:0.0\n",
            "iteration 426: weight=0.9999999998864106 loss:0.0\n",
            "iteration 427: weight=0.9999999998967566 loss:0.0\n",
            "iteration 428: weight=0.999999999906619 loss:0.0\n",
            "iteration 429: weight=0.9999999999159941 loss:0.0\n",
            "iteration 430: weight=0.999999999924881 loss:0.0\n",
            "iteration 431: weight=0.9999999999332814 loss:0.0\n",
            "iteration 432: weight=0.9999999999411993 loss:0.0\n",
            "iteration 433: weight=0.999999999948641 loss:0.0\n",
            "iteration 434: weight=0.9999999999556145 loss:0.0\n",
            "iteration 435: weight=0.9999999999621295 loss:0.0\n",
            "iteration 436: weight=0.9999999999681972 loss:0.0\n",
            "iteration 437: weight=0.9999999999738299 loss:0.0\n",
            "iteration 438: weight=0.9999999999790412 loss:0.0\n",
            "iteration 439: weight=0.9999999999838451 loss:0.0\n",
            "iteration 440: weight=0.999999999988257 loss:0.0\n",
            "iteration 441: weight=0.999999999992292 loss:0.0\n",
            "iteration 442: weight=0.9999999999959666 loss:0.0\n",
            "iteration 443: weight=0.9999999999992968 loss:0.0\n",
            "iteration 444: weight=1.0000000000022993 loss:0.0\n",
            "iteration 445: weight=1.0000000000049907 loss:0.0\n",
            "iteration 446: weight=1.0000000000073874 loss:0.0\n",
            "iteration 447: weight=1.0000000000095062 loss:0.0\n",
            "iteration 448: weight=1.0000000000113631 loss:0.0\n",
            "iteration 449: weight=1.0000000000129745 loss:0.0\n",
            "iteration 450: weight=1.000000000014356 loss:0.0\n",
            "iteration 451: weight=1.0000000000155231 loss:0.0\n",
            "iteration 452: weight=1.0000000000164908 loss:0.0\n",
            "iteration 453: weight=1.0000000000172737 loss:0.0\n",
            "iteration 454: weight=1.0000000000178861 loss:0.0\n",
            "iteration 455: weight=1.0000000000183418 loss:0.0\n",
            "iteration 456: weight=1.0000000000186535 loss:0.0\n",
            "iteration 457: weight=1.000000000018834 loss:0.0\n",
            "iteration 458: weight=1.0000000000188953 loss:0.0\n",
            "iteration 459: weight=1.000000000018849 loss:0.0\n",
            "iteration 460: weight=1.000000000018706 loss:0.0\n",
            "iteration 461: weight=1.0000000000184766 loss:0.0\n",
            "iteration 462: weight=1.0000000000181704 loss:0.0\n",
            "iteration 463: weight=1.0000000000177969 loss:0.0\n",
            "iteration 464: weight=1.0000000000173646 loss:0.0\n",
            "iteration 465: weight=1.0000000000168818 loss:0.0\n",
            "iteration 466: weight=1.000000000016356 loss:0.0\n",
            "iteration 467: weight=1.0000000000157943 loss:0.0\n",
            "iteration 468: weight=1.0000000000152032 loss:0.0\n",
            "iteration 469: weight=1.0000000000145888 loss:0.0\n",
            "iteration 470: weight=1.0000000000139566 loss:0.0\n",
            "iteration 471: weight=1.000000000013312 loss:0.0\n",
            "iteration 472: weight=1.0000000000126594 loss:0.0\n",
            "iteration 473: weight=1.0000000000120033 loss:0.0\n",
            "iteration 474: weight=1.0000000000113476 loss:0.0\n",
            "iteration 475: weight=1.0000000000106957 loss:0.0\n",
            "iteration 476: weight=1.0000000000100506 loss:0.0\n",
            "iteration 477: weight=1.0000000000094154 loss:0.0\n",
            "iteration 478: weight=1.000000000008792 loss:0.0\n",
            "iteration 479: weight=1.0000000000081832 loss:0.0\n",
            "iteration 480: weight=1.0000000000075906 loss:0.0\n",
            "iteration 481: weight=1.0000000000070157 loss:0.0\n",
            "iteration 482: weight=1.00000000000646 loss:0.0\n",
            "iteration 483: weight=1.0000000000059242 loss:0.0\n",
            "iteration 484: weight=1.0000000000054095 loss:0.0\n",
            "iteration 485: weight=1.0000000000049165 loss:0.0\n",
            "iteration 486: weight=1.000000000004446 loss:0.0\n",
            "iteration 487: weight=1.0000000000039981 loss:0.0\n",
            "iteration 488: weight=1.000000000003573 loss:0.0\n",
            "iteration 489: weight=1.0000000000031706 loss:0.0\n",
            "iteration 490: weight=1.0000000000027909 loss:0.0\n",
            "iteration 491: weight=1.0000000000024338 loss:0.0\n",
            "iteration 492: weight=1.000000000002099 loss:0.0\n",
            "iteration 493: weight=1.0000000000017861 loss:0.0\n",
            "iteration 494: weight=1.0000000000014946 loss:0.0\n",
            "iteration 495: weight=1.000000000001224 loss:0.0\n",
            "iteration 496: weight=1.0000000000009734 loss:0.0\n",
            "iteration 497: weight=1.0000000000007425 loss:0.0\n",
            "iteration 498: weight=1.0000000000005305 loss:0.0\n",
            "iteration 499: weight=1.0000000000003366 loss:0.0\n",
            "iteration 500: weight=1.0000000000001603 loss:0.0\n",
            "iteration 501: weight=1.0000000000000007 loss:0.0\n",
            "iteration 502: weight=0.9999999999998569 loss:0.0\n",
            "iteration 503: weight=0.9999999999997282 loss:0.0\n",
            "iteration 504: weight=0.9999999999996139 loss:0.0\n",
            "iteration 505: weight=0.9999999999995131 loss:0.0\n",
            "iteration 506: weight=0.9999999999994249 loss:0.0\n",
            "iteration 507: weight=0.9999999999993487 loss:0.0\n",
            "iteration 508: weight=0.9999999999992837 loss:0.0\n",
            "iteration 509: weight=0.9999999999992291 loss:0.0\n",
            "iteration 510: weight=0.9999999999991841 loss:0.0\n",
            "iteration 511: weight=0.9999999999991481 loss:0.0\n",
            "iteration 512: weight=0.9999999999991205 loss:0.0\n",
            "iteration 513: weight=0.9999999999991004 loss:0.0\n",
            "iteration 514: weight=0.9999999999990873 loss:0.0\n",
            "iteration 515: weight=0.9999999999990805 loss:0.0\n",
            "iteration 516: weight=0.9999999999990795 loss:0.0\n",
            "iteration 517: weight=0.9999999999990837 loss:0.0\n",
            "iteration 518: weight=0.9999999999990926 loss:0.0\n",
            "iteration 519: weight=0.9999999999991056 loss:0.0\n",
            "iteration 520: weight=0.9999999999991223 loss:0.0\n",
            "iteration 521: weight=0.9999999999991421 loss:0.0\n",
            "iteration 522: weight=0.9999999999991648 loss:0.0\n",
            "iteration 523: weight=0.9999999999991899 loss:0.0\n",
            "iteration 524: weight=0.999999999999217 loss:0.0\n",
            "iteration 525: weight=0.9999999999992457 loss:0.0\n",
            "iteration 526: weight=0.9999999999992758 loss:0.0\n",
            "iteration 527: weight=0.9999999999993069 loss:0.0\n",
            "iteration 528: weight=0.9999999999993388 loss:0.0\n",
            "iteration 529: weight=0.9999999999993711 loss:0.0\n",
            "iteration 530: weight=0.9999999999994037 loss:0.0\n",
            "iteration 531: weight=0.9999999999994365 loss:0.0\n",
            "iteration 532: weight=0.9999999999994691 loss:0.0\n",
            "iteration 533: weight=0.9999999999995014 loss:0.0\n",
            "iteration 534: weight=0.9999999999995333 loss:0.0\n",
            "iteration 535: weight=0.9999999999995646 loss:0.0\n",
            "iteration 536: weight=0.9999999999995952 loss:0.0\n",
            "iteration 537: weight=0.9999999999996251 loss:0.0\n",
            "iteration 538: weight=0.999999999999654 loss:0.0\n",
            "iteration 539: weight=0.999999999999682 loss:0.0\n",
            "iteration 540: weight=0.999999999999709 loss:0.0\n",
            "iteration 541: weight=0.999999999999735 loss:0.0\n",
            "iteration 542: weight=0.9999999999997599 loss:0.0\n",
            "iteration 543: weight=0.9999999999997836 loss:0.0\n",
            "iteration 544: weight=0.9999999999998062 loss:0.0\n",
            "iteration 545: weight=0.9999999999998276 loss:0.0\n",
            "iteration 546: weight=0.9999999999998478 loss:0.0\n",
            "iteration 547: weight=0.9999999999998669 loss:0.0\n",
            "iteration 548: weight=0.9999999999998849 loss:0.0\n",
            "iteration 549: weight=0.9999999999999016 loss:0.0\n",
            "iteration 550: weight=0.9999999999999173 loss:0.0\n",
            "iteration 551: weight=0.9999999999999318 loss:0.0\n",
            "iteration 552: weight=0.9999999999999454 loss:0.0\n",
            "iteration 553: weight=0.9999999999999578 loss:0.0\n",
            "iteration 554: weight=0.9999999999999692 loss:0.0\n",
            "iteration 555: weight=0.9999999999999798 loss:0.0\n",
            "iteration 556: weight=0.9999999999999893 loss:0.0\n",
            "iteration 557: weight=0.999999999999998 loss:0.0\n",
            "iteration 558: weight=1.0000000000000058 loss:0.0\n",
            "iteration 559: weight=1.0000000000000129 loss:0.0\n",
            "iteration 560: weight=1.000000000000019 loss:0.0\n",
            "iteration 561: weight=1.0000000000000246 loss:0.0\n",
            "iteration 562: weight=1.0000000000000295 loss:0.0\n",
            "iteration 563: weight=1.0000000000000338 loss:0.0\n",
            "iteration 564: weight=1.0000000000000373 loss:0.0\n",
            "iteration 565: weight=1.0000000000000404 loss:0.0\n",
            "iteration 566: weight=1.0000000000000429 loss:0.0\n",
            "iteration 567: weight=1.0000000000000449 loss:0.0\n",
            "iteration 568: weight=1.0000000000000464 loss:0.0\n",
            "iteration 569: weight=1.0000000000000475 loss:0.0\n",
            "iteration 570: weight=1.0000000000000482 loss:0.0\n",
            "iteration 571: weight=1.0000000000000486 loss:0.0\n",
            "converged after 572 iterations\n"
          ]
        }
      ],
      "source": [
        "# Initialize w_0 and b_0 with random values using function\n",
        "w_0 = random.random()\n",
        "b_0 = random.random()\n",
        "adam = AdamOptim()\n",
        "# Initialize iteration with 1\n",
        "t = 1\n",
        "converged = False\n",
        "iter_adam, loss_adam= [], []\n",
        "\n",
        "while not converged:\n",
        "    dw = grad_function(w_0)\n",
        "    db = grad_function(b_0)\n",
        "    w_0_old = w_0\n",
        "    w_0, b_0, l = adam.update(t,w=w_0, b=b_0, dw=dw, db=db)\n",
        "    if check_convergence(w_0, w_0_old):\n",
        "        print('converged after '+str(t)+' iterations')\n",
        "        break\n",
        "    else:\n",
        "      iter_adam.append(t)\n",
        "      loss_adam.append(l)\n",
        "      print('iteration '+str(t)+': weight='+str(w_0)+\" loss:\"+str(l))\n",
        "      t+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "CbtbhiFoofNh",
        "outputId": "abf323e3-a9a4-4acb-d80a-6c950f7adc3f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAd/ElEQVR4nO3de5CddZ3n8fcnfTmddHdCLs0tFxIgLAYVsNqoCzqOcomXSlgHV1B3mVqqskyRFWVmnTBO4U6mrELc1Rl3MyupMuvujhgRxp0uJ05ERB0vQBoIgQQjTbgkEU1LroTcOvnuH8+vw5OTk+R0up+c7j6fV9WpnOf3PM/p7wNNPvx+v+c8P0UEZmZm5cbUugAzMxueHBBmZlaRA8LMzCpyQJiZWUUOCDMzq6ix1gUMlSlTpsTMmTNrXYaZ2Yjy+OOP/z4iOirtGzUBMXPmTLq7u2tdhpnZiCLppePt8xCTmZlV5IAwM7OKHBBmZlaRA8LMzCpyQJiZWUWFBoSkeZI2SOqRtPgEx/2RpJDUmWu7I523QdK1RdZpZmbHKuw2V0kNwFLgamAzsFpSV0SsLzuuHbgNeDTXNge4AbgEOBf4oaSLIuJQUfWamdnRiuxBzAV6ImJjRBwAVgALKhz318AXgX25tgXAiojYHxEvAD3p84bc7n0H+cqDv2bNph1FfLyZ2YhVZEBMBTbltjentiMkvQ2YHhH/NNBz0/kLJXVL6u7t7T2lIg8dDv72oed44qXtp3S+mdloVbNJakljgC8Df3qqnxERyyKiMyI6OzoqflP8pNpK2Sjb7n19p1qGmdmoVOSjNrYA03Pb01Jbv3bgzcCPJQGcDXRJml/FuUOmsWEMrc0N7Np3sIiPNzMbsYrsQawGZkuaJamZbNK5q39nROyMiCkRMTMiZgKPAPMjojsdd4OkkqRZwGzgsaIKbW9pYrcDwszsKIX1ICKiT9IiYBXQACyPiHWSlgDdEdF1gnPXSboPWA/0AbcWeQfT+LGN7NrrISYzs7xCn+YaESuBlWVtdx7n2PeWbX8B+EJhxeW0tzSxe797EGZmef4mNTC+pdGT1GZmZRwQZD2IXXvdgzAzy3NAAO3uQZiZHcMBAYwf28SufQeJiFqXYmY2bDggyHoQBw8F+/sO17oUM7NhwwEBjG9pAvCX5czMchwQZD0IwN+FMDPLcUDwRg/C36Y2M3uDA4Lsm9QAu3wnk5nZEQ4Isu9BgHsQZmZ5Dghyk9SegzAzO8IBwRuT1O5BmJm9wQEBjGtuYIzgtf3uQZiZ9XNAAJJoLflxG2ZmeQ6IpL3U6B6EmVmOAyJpa2lkjwPCzOyIQgNC0jxJGyT1SFpcYf8tkp6WtEbSzyTNSe0zJe1N7Wskfa3IOgFa3YMwMztKYSvKSWoAlgJXA5uB1ZK6ImJ97rB7I+Jr6fj5wJeBeWnf8xFxWVH1lWvzHISZ2VGK7EHMBXoiYmNEHABWAAvyB0TErtxmK1Cz5223e4jJzOwoRQbEVGBTbntzajuKpFslPQ/cDXwqt2uWpCcl/UTSuyv9AEkLJXVL6u7t7R1Usa3NHmIyM8ur+SR1RCyNiAuAPwf+MjW/AsyIiMuB24F7JY2vcO6yiOiMiM6Ojo5B1dHW0shrHmIyMzuiyIDYAkzPbU9LbcezArgOICL2R8Sr6f3jwPPARQXVCaTbXA/0eVU5M7OkyIBYDcyWNEtSM3AD0JU/QNLs3OaHgOdSe0ea5EbS+cBsYGOBtdJaaiQCXj9wqMgfY2Y2YhR2F1NE9ElaBKwCGoDlEbFO0hKgOyK6gEWSrgIOAtuBm9Lp7wGWSDoIHAZuiYhtRdUK2RATZI/baC0V9o/FzGzEKPRvwohYCawsa7sz9/6245z3APBAkbWVayu9ERBnnc4fbGY2TNV8knq4OBIQnqg2MwMcEEfkexBmZuaAOCI/B2FmZg6IIzzEZGZ2NAdE4iEmM7OjOSASDzGZmR3NAZGUGhtoapADwswscUDktJX8PCYzs34OiJy2Fj/R1cysnwMip63U5IAwM0scEDltpQYPMZmZJQ6InDavS21mdoQDIqetpcnLjpqZJQ6InLZSA7sdEGZmgAPiKL7N1czsDQ6InLZSE3sPHuLQYS87amZWaEBImidpg6QeSYsr7L9F0tOS1kj6maQ5uX13pPM2SLq2yDr7tZYaAD9uw8wMCgyItKb0UuADwBzgxnwAJPdGxFsi4jLgbuDL6dw5ZGtYXwLMA/6uf43qIrX7eUxmZkcU2YOYC/RExMaIOACsABbkD4iIXbnNVqB/bGcBsCIi9kfEC0BP+rxCtZWaAHwnk5kZxa5JPRXYlNveDLyj/CBJtwK3A83A+3LnPlJ27tQK5y4EFgLMmDFj0AX3DzHt9kS1mVntJ6kjYmlEXAD8OfCXAzx3WUR0RkRnR0fHoGvxEJOZ2RuKDIgtwPTc9rTUdjwrgOtO8dwh4SEmM7M3FBkQq4HZkmZJaiabdO7KHyBpdm7zQ8Bz6X0XcIOkkqRZwGzgsQJrBXJ3MXmIycysuDmIiOiTtAhYBTQAyyNinaQlQHdEdAGLJF0FHAS2Azelc9dJug9YD/QBt0bEoaJq7deeehD+NrWZWbGT1ETESmBlWdudufe3neDcLwBfKK66Y/X3IDzEZGY2DCaph5PGhjG0NI3xJLWZGQ6IY7SVmnybq5kZDohjtHvZUTMzwAFxjNZSg+cgzMxwQBzDj/w2M8s4IMq0lZp8m6uZGQ6IY7R5iMnMDHBAHKPNk9RmZoAD4hhtpSbPQZiZ4YA4RlupgQOHDrO/r/Ane5iZDWsOiDJtpezpI3v2OyDMrL45IMq0tWQP7PMwk5nVOwdEmbb+R357otrM6pwDokz/okEOCDOrdw6IMm1Hlh09WONKzMxqq9CAkDRP0gZJPZIWV9h/u6T1ktZKekjSebl9hyStSa+u8nOL8sYQkyepzay+FbZgkKQGYClwNbAZWC2pKyLW5w57EuiMiNcl/QlwN/CxtG9vRFxWVH3Hc2SIyZPUZlbniuxBzAV6ImJjRBwAVgAL8gdExMMR8XrafASYVmA9VfEQk5lZpsiAmApsym1vTm3HczPw/dx2i6RuSY9Iuq7SCZIWpmO6e3t7B18xMK7JQ0xmZlDwmtTVkvRJoBP4g1zzeRGxRdL5wI8kPR0Rz+fPi4hlwDKAzs7OGIpaxoyRH/ltZkaxPYgtwPTc9rTUdhRJVwGfA+ZHxP7+9ojYkv7cCPwYuLzAWo/SVmr0EJOZ1b0iA2I1MFvSLEnNwA3AUXcjSbocuIcsHLbm2idKKqX3U4ArgPzkdqFaSw3+HoSZ1b3Chpgiok/SImAV0AAsj4h1kpYA3RHRBXwJaAO+Iwng5YiYD7wJuEfSYbIQu6vs7qdCtbU0eQ7CzOpeoXMQEbESWFnWdmfu/VXHOe8XwFuKrO1E2kuNvLbPQ0xmVt/8TeoKPMRkZuaAqKit1OTHfZtZ3XNAVNDe0shuDzGZWZ1zQFTQP8QUMSRfrTAzG5EcEBW0lZo4HLDv4OFal2JmVjMOiAr6n8e021+WM7M65oCo4Mgjv/24DTOrYw6ICvof+e07mcysnjkgKmgreYjJzMwBUUF/QHiIyczqmQOigv5J6j0HHBBmVr+qCghJrZLGpPcXSZovqanY0mrHPQgzs+p7ED8lW+FtKvAD4N8B3yiqqFp7Yw7CAWFm9avagFBaO/ojwN9FxEeBS4orq7ZamsbQMEbscUCYWR2rOiAkvQv4BPBPqa2hmJJqT/Kyo2Zm1QbEp4E7gO+mRX/OBx4urqzaays1eojJzOpaVQERET+JiPkR8cU0Wf37iPjUyc6TNE/SBkk9khZX2H+7pPWS1kp6SNJ5uX03SXouvW4a0FUNgbZSo4eYzKyuVXsX072SxktqBZ4B1kv6zyc5pwFYCnwAmAPcKGlO2WFPAp0R8VbgfuDudO4k4PPAO4C5wOclTaz+sgavraWR3R5iMrM6Vu0Q05yI2AVcB3wfmEV2J9OJzAV6ImJjRBwAVgAL8gdExMNp8hvgEWBaen8t8GBEbIuI7cCDwLwqax0S7Q4IM6tz1QZEU/rew3VAV0QcBE62WMJUYFNue3NqO56bycKn6nMlLZTULam7t7f3JOUMzPiWJnbu9aM2zKx+VRsQ9wAvAq3AT9Ncwa6hKkLSJ4FO4EsDOS8ilkVEZ0R0dnR0DFU5AEwY64Aws/pW7ST1VyNiakR8MDIvAX94ktO2ANNz29NS21EkXQV8DpgfEfsHcm6RJoxtYve+gxw+7FXlzKw+VTtJPUHSl/uHcyT9N7LexImsBmZLmiWpGbgB6Cr73MvJeifzI2Jrbtcq4BpJE9Pk9DWp7bSZMDZbVe41P4/JzOpUtUNMy4HdwL9Nr13A/zrRCRHRBywi+4v9WeC+9B2KJZLmp8O+BLQB35G0RlJXOncb8NdkIbMaWJLaTpsJY7NHTe183cNMZlafGqs87oKI+KPc9l9JWnOykyJiJbCyrO3O3PurTnDucrJgqonx/QGx9+BRY11mZvWi2h7EXklX9m9IugLYW0xJw0N/D2KXJ6rNrE5V24O4Bfg/kiak7e3Aaf928+k0fmz2j2bXPgeEmdWnqgIiIp4CLpU0Pm3vkvRpYG2RxdXShNwQk5lZPRrQinIRsSt9oxrg9gLqGTYcEGZW7waz5KiGrIphqK3USMMYOSDMrG4NJiBG9TfIJDG+pdEBYWZ164RzEJJ2UzkIBIwtpKJhJHvchr8oZ2b16YQBERHtp6uQ4Wj82Cbf5mpmdWswQ0yjnh/YZ2b1zAFxAu5BmFk9c0CcgHsQZlbPHBAn0B8QEaP6hi0zs4ocECcwYWwTfYeDvQcP1boUM7PTzgFxAuNb/G1qM6tfDogT8OM2zKyeOSBOwIsGmVk9KzQgJM2TtEFSj6TFFfa/R9ITkvokXV+271BaZe7ISnOnm3sQZlbPql0PYsAkNQBLgauBzcBqSV0RsT532MvAHwN/VuEj9kbEZUXVV40zxmUBscMBYWZ1qLCAAOYCPRGxEUDSCmABcCQgIuLFtO9wgXWcsomtzQDseP1AjSsxMzv9ihximgpsym1vTm3VapHULekRSddVOkDSwnRMd29v72Bqrai1uYHmhjG8uscBYWb1ZzhPUp8XEZ3Ax4G/kXRB+QERsSwiOiOis6OjY8gLkMSk1ma2OyDMrA4VGRBbgOm57WmprSoRsSX9uRH4MXD5UBZXrYmtzWxzQJhZHSoyIFYDsyXNktQM3ABUdTeSpImSSun9FOAKcnMXp9NkB4SZ1anCAiIi+oBFwCrgWeC+iFgnaYmk+QCS3i5pM/BR4B5J69LpbwK6JT0FPAzcVXb302njHoSZ1asi72IiIlYCK8va7sy9X0029FR+3i+AtxRZW7XcgzCzejWcJ6mHhYnjmtm1r4+Dh4blnbhmZoVxQJzEpLbsuxDb/V0IM6szDoiTmDQuCwgPM5lZvXFAnMSkVgeEmdUnB8RJ9AfE9j1+HpOZ1RcHxEm80YPYX+NKzMxOLwfESfQ/0XWbexBmVmccECfR1DCG8S2N7kGYWd1xQFRhcluJbV5VzszqjAOiChPHNbkHYWZ1xwFRhUmtJc9BmFndcUBUYVKrexBmVn8cEFWY3FZi254DREStSzEzO20cEFXoaCtx8FCwwxPVZlZHHBBVOHN8CYCtuz3MZGb1o9CAkDRP0gZJPZIWV9j/HklPSOqTdH3ZvpskPZdeNxVZ58mc2d4CwNbd+2pZhpnZaVVYQEhqAJYCHwDmADdKmlN22MvAHwP3lp07Cfg88A5gLvB5SROLqvVkzmxPPYhd7kGYWf0osgcxF+iJiI0RcQBYASzIHxARL0bEWqB8NZ5rgQcjYltEbAceBOYVWOsJdbR7iMnM6k+RATEV2JTb3pzaij53yLWWGmltbvAQk5nVlRE9SS1poaRuSd29vb2F/qwzx7e4B2FmdaXIgNgCTM9tT0ttQ3ZuRCyLiM6I6Ozo6DjlQqvR0V6i13MQZlZHigyI1cBsSbMkNQM3AF1VnrsKuEbSxDQ5fU1qq5kz20seYjKzulJYQEREH7CI7C/2Z4H7ImKdpCWS5gNIerukzcBHgXskrUvnbgP+mixkVgNLUlvNnNnuISYzqy+NRX54RKwEVpa13Zl7v5ps+KjSucuB5UXWNxBnjS/x+oFD7N53kPaWplqXY2ZWuBE9SX06nXPGWABe2elhJjOrDw6IKp07Ifs29W927K1xJWZmp4cDokrnph7Eb3a4B2Fm9cEBUaUz20uMEbyy0z0IM6sPDogqNTaM4ezxLWzxEJOZ1QkHxACcc8ZYXvEQk5nVCQfEAJx7xlgPMZlZ3XBADMC5E1r4zc59XnrUzOqCA2IAzj1jLAf6DvP71w7UuhQzs8I5IAZgxqRxALy87fUaV2JmVjwHxACcNzkLiJde3VPjSszMiueAGIBpE8cxRvDSq+5BmNno54AYgObGMZwzYax7EGZWFxwQAzRzyjhe8hyEmdUBB8QAzZjU6iEmM6sLDogBmjl5HNv2HGDXvoO1LsXMrFCFBoSkeZI2SOqRtLjC/pKkb6f9j0qamdpnStoraU16fa3IOgdi5pRWAF7o9TyEmY1uhQWEpAZgKfABYA5wo6Q5ZYfdDGyPiAuBrwBfzO17PiIuS69biqpzoGaf2QbAc1tfq3ElZmbFKrIHMRfoiYiNEXEAWAEsKDtmAfC/0/v7gfdLUoE1Ddp5k1tpbhzDr3+3u9almJkVqsiAmApsym1vTm0Vj4mIPmAnMDntmyXpSUk/kfTuSj9A0kJJ3ZK6e3t7h7b642gYIy7oaHNAmNmoN1wnqV8BZkTE5cDtwL2SxpcfFBHLIqIzIjo7OjpOW3EXndXGc7/zEJOZjW5FBsQWYHpue1pqq3iMpEZgAvBqROyPiFcBIuJx4HngogJrHZCLzmpny469vLa/r9almJkVpsiAWA3MljRLUjNwA9BVdkwXcFN6fz3wo4gISR1pkhtJ5wOzgY0F1jogF53VDsCG33qYycxGr8ICIs0pLAJWAc8C90XEOklLJM1Ph30dmCyph2woqf9W2PcAayWtIZu8viUithVV60C9eWo22vX05h01rsTMrDiNRX54RKwEVpa13Zl7vw/4aIXzHgAeKLK2wTh7fAtntpdYu3lnrUsxMyvMcJ2kHtYk8dZpZ7DGPQgzG8UcEKfo0mkT2Ni7x4/cMLNRywFxii6dfgYAazd5mMnMRicHxCm6fMYZNIwRv9z4+1qXYmZWCAfEKWpvaeLSaRP4ec+rtS7FzKwQDohBuOLCKazdvMPzEGY2KjkgBuFdF0zmcMAvn3cvwsxGHwfEIHSeN4n2lkZWrfttrUsxMxtyDohBaG4cwzVzzubB9b9jf9+hWpdjZjakHBCD9KG3ns3ufX387DnfzWRmo4sDYpCuvLCDya3NfOuxl2tdipnZkHJADFJz4xg+/o4ZPPSrrbz0qtepNrPRwwExBD75zvNokPjaT4bNE8nNzAbNATEEzhrfwiffeR7fXv0yv/rtrlqXY2Y2JBwQQ+S298+mvaWJz96/1nc0mdmo4IAYIhNbm7n7+reydvNO/uIfnuHQ4ah1SWZmg1JoQEiaJ2mDpB5JiyvsL0n6dtr/qKSZuX13pPYNkq4tss6hcu0lZ/OZqy7igSc2c8vfP862PQdqXZKZ2SkrbEW5tKb0UuBqYDOwWlJXRKzPHXYzsD0iLpR0A/BF4GOS5pCtYX0JcC7wQ0kXRcSwH7u57arZtLU0ctf3n+UP7n6Yj3ZO5/1vOpM3T53AhLFNtS7PzKxqRS45OhfoiYiNAJJWAAuAfEAsAP5Len8/8D8kKbWviIj9wAtpzeq5wC8LrHfI3HzlLK68cApffeg5/v6Rl1j+8xcAaC81Mq7UwNimBhobPLpX71TrAmzUuPic8fz3Gy8f8s8tMiCmApty25uBdxzvmIjok7QTmJzaHyk7d2r5D5C0EFgIMGPGjCErfCj8q7PbWfqJt7Fnfx+PvvAqPVtf4zc79rHv4CFeP3DIcxR1LvC/fxs60yeOLeRziwyIwkXEMmAZQGdn57D8L6611Mj7Lj6L9118Vq1LMTMbkCLHObYA03Pb01JbxWMkNQITgFerPNfMzApUZECsBmZLmiWpmWzSuavsmC7gpvT+euBHERGp/YZ0l9MsYDbwWIG1mplZmcKGmNKcwiJgFdAALI+IdZKWAN0R0QV8Hfi/aRJ6G1mIkI67j2xCuw+4dSTcwWRmNpoo+x/2ka+zszO6u7trXYaZ2Ygi6fGI6Ky0z/damplZRQ4IMzOryAFhZmYVOSDMzKyiUTNJLakXeOkUT58CjMZFpUfjdfmaRo7ReF2j8ZrOi4iOSjtGTUAMhqTu483ij2Sj8bp8TSPHaLyu0XhNJ+IhJjMzq8gBYWZmFTkgMstqXUBBRuN1+ZpGjtF4XaPxmo7LcxBmZlaRexBmZlaRA8LMzCqq+4CQNE/SBkk9khbXup5qSVouaaukZ3JtkyQ9KOm59OfE1C5JX03XuFbS22pX+fFJmi7pYUnrJa2TdFtqH+nX1SLpMUlPpev6q9Q+S9Kjqf5vp8fikx5z/+3U/qikmbWs/0QkNUh6UtL30vaIviZJL0p6WtIaSd2pbUT//g1GXQeEpAZgKfABYA5wo6Q5ta2qat8A5pW1LQYeiojZwENpG7Lrm51eC4H/eZpqHKg+4E8jYg7wTuDW9O9jpF/XfuB9EXEpcBkwT9I7gS8CX4mIC4HtwM3p+JuB7an9K+m44eo24Nnc9mi4pj+MiMty33cY6b9/py4i6vYFvAtYldu+A7ij1nUNoP6ZwDO57Q3AOen9OcCG9P4e4MZKxw3nF/CPwNWj6bqAccATZOuz/x5oTO1HfhfJ1lB5V3rfmI5TrWuvcC3TyP7CfB/wPUCj4JpeBKaUtY2a37+Bvuq6BwFMBTbltjentpHqrIh4Jb3/LdC/EPaIu840BHE58Cij4LrSUMwaYCvwIPA8sCMi+tIh+dqPXFfavxOYfHorrsrfAJ8FDqftyYz8awrgB5Iel7QwtY34379TVdiKclZbERGSRuQ9zJLagAeAT0fELklH9o3U64psRcTLJJ0BfBe4uMYlDYqkDwNbI+JxSe+tdT1D6MqI2CLpTOBBSb/K7xypv3+nqt57EFuA6bntaaltpPqdpHMA0p9bU/uIuU5JTWTh8M2I+IfUPOKvq19E7AAeJht+OUNS//+k5Ws/cl1p/wTg1dNc6slcAcyX9CKwgmyY6W8Z2ddERGxJf24lC/K5jKLfv4Gq94BYDcxOd140k62J3VXjmgajC7gpvb+JbAy/v/3fp7su3gnszHWZhw1lXYWvA89GxJdzu0b6dXWkngOSxpLNqzxLFhTXp8PKr6v/eq8HfhRpkHu4iIg7ImJaRMwk++/mRxHxCUbwNUlqldTe/x64BniGEf77Nyi1ngSp9Qv4IPBrsjHhz9W6ngHU/S3gFeAg2djnzWRjug8BzwE/BCalY0V2t9bzwNNAZ63rP841XUk2BrwWWJNeHxwF1/VW4Ml0Xc8Ad6b284HHgB7gO0Aptbek7Z60//xaX8NJru+9wPdG+jWl2p9Kr3X9fx+M9N+/wbz8qA0zM6uo3oeYzMzsOBwQZmZWkQPCzMwqckCYmVlFDggzM6vIAWGWSHot/TlT0seH+LP/omz7F0P5+WZFcECYHWsmMKCAyH17+HiOCoiI+NcDrMnstHNAmB3rLuDdaU2Az6QH7X1J0ur03P//CCDpvZL+RVIXsD61/b/0oLd1/Q97k3QXMDZ93jdTW39vRemzn0nrEHws99k/lnS/pF9J+mb6pjmS7lK2ZsZaSf/1tP/Tsbrhh/WZHWsx8GcR8WGA9Bf9zoh4u6QS8HNJP0jHvg14c0S8kLb/Q0RsS4/UWC3pgYhYLGlRRFxW4Wd9hGyNiEuBKemcn6Z9lwOXAL8Bfg5cIelZ4N8AF0dE9D/Cw6wI7kGYndw1ZM/cWUP2+PHJZIvEADyWCweAT0l6CniE7EFuszmxK4FvRcShiPgd8BPg7bnP3hwRh8keOzKT7DHZ+4CvS/oI8Pqgr87sOBwQZicn4D9FtsrYZRExKyL6exB7jhyUPfb6KrKFcS4le/5SyyB+7v7c+0NkC/H0kT1h9H7gw8A/D+LzzU7IAWF2rN1Ae257FfAn6VHkSLooPe2z3ASyZTVfl3Qx2bKp/Q72n1/mX4CPpXmODuA9ZA+zqyitlTEhIlYCnyEbmjIrhOcgzI61FjiUhoq+QbbOwUzgiTRR3AtcV+G8fwZuSfMEG8iGmfotA9ZKeiKyx2L3+y7Z2hBPkT3J9rMR8dsUMJW0A/8oqYWsZ3P7qV2i2cn5aa5mZlaRh5jMzKwiB4SZmVXkgDAzs4ocEGZmVpEDwszMKnJAmJlZRQ4IMzOr6P8D7zOVJMta7c4AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(iter_adam, loss_adam)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pDs2vPf4wkMc"
      },
      "outputs": [],
      "source": [
        "class RMSProp():\n",
        "    def __init__(self, eta=0.01, beta=0.9, epsilon=1e-8):\n",
        "      self.v_dw, self.v_db = 0, 0\n",
        "      self.beta = beta\n",
        "      self.epsilon = epsilon\n",
        "      self.eta = eta\n",
        "\n",
        "    def update(self, t, w, b, dw, db):\n",
        "      #rmsprop\n",
        "      self.v_dw = self.beta*self.v_dw + ((1-self.beta)*(dw**2))\n",
        "      self.v_db = self.beta*self.v_db + ((1-self.beta)*(db**2))\n",
        "      \n",
        "      ## bias correction\n",
        "      v_dw_corr = self.v_dw/(1-self.beta)\n",
        "      v_db_corr = self.v_db/(1-self.beta)\n",
        "\n",
        "      ## update weights and biases\n",
        "      w = w- self.eta*(dw/(np.sqrt(v_dw_corr)+self.epsilon))\n",
        "      b = b- self.eta*(db/(np.sqrt(v_db_corr)+self.epsilon))\n",
        "      loss= loss_function(b)\n",
        "      return w, b, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA6cHgfWx4Mx",
        "outputId": "ceb19aeb-3135-40b8-e6de-93acf130ccc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration 1: weight=0.3823466184977722 loss: 0.4809050742157295\n",
            "iteration 2: weight=0.3895459352732548 loss: 0.47096354337887103\n",
            "iteration 3: weight=0.3955460849737819 loss: 0.4627530569453576\n",
            "iteration 4: weight=0.4008537193495976 loss: 0.45554753544145077\n",
            "iteration 5: weight=0.40570350172432823 loss: 0.4490108620282732\n",
            "iteration 6: weight=0.4102260399084933 loss: 0.4429560748116146\n",
            "iteration 7: weight=0.41450251296677215 loss: 0.43726705378560915\n",
            "iteration 8: weight=0.4185872595918865 loss: 0.4318661417578298\n",
            "iteration 9: weight=0.42251864260509936 loss: 0.4266985717162901\n",
            "iteration 10: weight=0.4263248545250165 loss: 0.42172414573431205\n",
            "iteration 11: weight=0.4300272714117198 loss: 0.4169124280400105\n",
            "iteration 12: weight=0.43364251076671523 loss: 0.4122397955613174\n",
            "iteration 13: weight=0.43718375648921015 loss: 0.4076875389780671\n",
            "iteration 14: weight=0.4406616461722074 loss: 0.40324059105403876\n",
            "iteration 15: weight=0.4440848851317423 loss: 0.39888664663125273\n",
            "iteration 16: weight=0.4474606833148683 loss: 0.3946155364879159\n",
            "iteration 17: weight=0.45079507369585414 loss: 0.39041877106048317\n",
            "iteration 18: weight=0.4540931491740065 loss: 0.38628920098242037\n",
            "iteration 19: weight=0.4573592420729262 loss: 0.3822207599000498\n",
            "iteration 20: weight=0.4605970623568106 loss: 0.3782082664686106\n",
            "iteration 21: weight=0.463809805596475 loss: 0.3742472697162722\n",
            "iteration 22: weight=0.4670002383972102 loss: 0.37033392672261156\n",
            "iteration 23: weight=0.47017076678095493 loss: 0.36646490473902693\n",
            "iteration 24: weight=0.4733234915007081 loss: 0.362637302049054\n",
            "iteration 25: weight=0.4764602532123271 loss: 0.3588485833752517\n",
            "iteration 26: weight=0.4795826696846789 loss: 0.3550965267057966\n",
            "iteration 27: weight=0.48269216669497644 loss: 0.3513791791793788\n",
            "iteration 28: weight=0.48579000386736343 loss: 0.347694820224154\n",
            "iteration 29: weight=0.48887729642619665 loss: 0.34404193055727017\n",
            "iteration 30: weight=0.4919550336216617 loss: 0.34041916595794286\n",
            "iteration 31: weight=0.4950240944240995 loss: 0.3368253349582008\n",
            "iteration 32: weight=0.49808526096056216 loss: 0.33325937977155184\n",
            "iteration 33: weight=0.501139230072622 loss: 0.3297203599153069\n",
            "iteration 34: weight=0.5041866233011347 loss: 0.32620743808744845\n",
            "iteration 35: weight=0.5072279955462923 loss: 0.322719867941225\n",
            "iteration 36: weight=0.510263842606061 loss: 0.3192569834655564\n",
            "iteration 37: weight=0.5132946077601738 loss: 0.31581818973091214\n",
            "iteration 38: weight=0.5163206875380991 loss: 0.3124029548015974\n",
            "iteration 39: weight=0.5193424367862666 loss: 0.3090108026486268\n",
            "iteration 40: weight=0.5223601731310799 loss: 0.305641306924316\n",
            "iteration 41: weight=0.5253741809189593 loss: 0.302294085481704\n",
            "iteration 42: weight=0.5283847147021267 loss: 0.29896879553994726\n",
            "iteration 43: weight=0.5313920023285148 loss: 0.295665129411699\n",
            "iteration 44: weight=0.5343962476856232 loss: 0.2923828107208156\n",
            "iteration 45: weight=0.5373976331410146 loss: 0.2891215910490106\n",
            "iteration 46: weight=0.540396321716177 loss: 0.2858812469586778\n",
            "iteration 47: weight=0.5433924590254634 loss: 0.28266157734634356\n",
            "iteration 48: weight=0.5463861750075814 loss: 0.2794624010873257\n",
            "iteration 49: weight=0.549377585473515 loss: 0.27628355493736345\n",
            "iteration 50: weight=0.5523667934916986 loss: 0.2731248916614033\n",
            "iteration 51: weight=0.5553538906286468 loss: 0.26998627836350664\n",
            "iteration 52: weight=0.5583389580610026 loss: 0.26686759499508916\n",
            "iteration 53: weight=0.5613220675730275 loss: 0.2637687330214933\n",
            "iteration 54: weight=0.5643032824518897 loss: 0.2606895942293128\n",
            "iteration 55: weight=0.5672826582916594 loss: 0.2576300896589764\n",
            "iteration 56: weight=0.5702602437156563 loss: 0.2545901386489179\n",
            "iteration 57: weight=0.5732360810257007 loss: 0.2515696679792422\n",
            "iteration 58: weight=0.5762102067858558 loss: 0.2485686111041846\n",
            "iteration 59: weight=0.5791826523474094 loss: 0.24558690746386724\n",
            "iteration 60: weight=0.5821534443211007 loss: 0.24262450186692686\n",
            "iteration 61: weight=0.5851226050019482 loss: 0.23968134393651763\n",
            "iteration 62: weight=0.5880901527514583 loss: 0.23675738761302245\n",
            "iteration 63: weight=0.5910561023414858 loss: 0.23385259070753084\n",
            "iteration 64: weight=0.5940204652635667 loss: 0.23096691450078422\n",
            "iteration 65: weight=0.5969832500071436 loss: 0.22810032338286512\n",
            "iteration 66: weight=0.599944462309748 loss: 0.22525278452940256\n",
            "iteration 67: weight=0.6029041053818875 loss: 0.22242426761052492\n",
            "iteration 68: weight=0.6058621801091049 loss: 0.2196147445291815\n",
            "iteration 69: weight=0.6088186852334201 loss: 0.21682418918581559\n",
            "iteration 70: weight=0.6117736175161468 loss: 0.2140525772666837\n",
            "iteration 71: weight=0.6147269718838687 loss: 0.21129988605340033\n",
            "iteration 72: weight=0.6176787415591827 loss: 0.20856609425153672\n",
            "iteration 73: weight=0.6206289181776542 loss: 0.20585118183632822\n",
            "iteration 74: weight=0.6235774918922846 loss: 0.2031551299137473\n",
            "iteration 75: weight=0.6265244514666597 loss: 0.20047792059537295\n",
            "iteration 76: weight=0.6294697843578324 loss: 0.19781953688565657\n",
            "iteration 77: weight=0.6324134767898869 loss: 0.19517996258031833\n",
            "iteration 78: weight=0.6353555138190362 loss: 0.19255918217474588\n",
            "iteration 79: weight=0.6382958793910203 loss: 0.18995718078137624\n",
            "iteration 80: weight=0.6412345563914955 loss: 0.18737394405514762\n",
            "iteration 81: weight=0.6441715266900359 loss: 0.18480945812620153\n",
            "iteration 82: weight=0.647106771178304 loss: 0.1822637095390971\n",
            "iteration 83: weight=0.6500402698028941 loss: 0.17973668519787311\n",
            "iteration 84: weight=0.6529720015932979 loss: 0.1772283723163648\n",
            "iteration 85: weight=0.6559019446853974 loss: 0.17473875837323638\n",
            "iteration 86: weight=0.6588300763408474 loss: 0.1722678310712502\n",
            "iteration 87: weight=0.6617563729626741 loss: 0.16981557830033667\n",
            "iteration 88: weight=0.6646808101073783 loss: 0.1673819881040759\n",
            "iteration 89: weight=0.6676033624938058 loss: 0.16496704864924128\n",
            "iteration 90: weight=0.6705240040090134 loss: 0.16257074819808703\n",
            "iteration 91: weight=0.6734427077113391 loss: 0.16019307508309777\n",
            "iteration 92: weight=0.6763594458308582 loss: 0.15783401768394234\n",
            "iteration 93: weight=0.6792741897673856 loss: 0.15549356440640283\n",
            "iteration 94: weight=0.6821869100861684 loss: 0.15317170366306976\n",
            "iteration 95: weight=0.6850975765113909 loss: 0.15086842385561872\n",
            "iteration 96: weight=0.6880061579176021 loss: 0.14858371335849785\n",
            "iteration 97: weight=0.6909126223191575 loss: 0.1463175605038748\n",
            "iteration 98: weight=0.6938169368577553 loss: 0.1440699535677068\n",
            "iteration 99: weight=0.6967190677881336 loss: 0.14184088075680945\n",
            "iteration 100: weight=0.6996189804619848 loss: 0.1396303301968138\n",
            "iteration 101: weight=0.7025166393101295 loss: 0.13743828992090878\n",
            "iteration 102: weight=0.7054120078229873 loss: 0.13526474785928044\n",
            "iteration 103: weight=0.7083050485293656 loss: 0.13310969182916388\n",
            "iteration 104: weight=0.7111957229735861 loss: 0.1309731095254345\n",
            "iteration 105: weight=0.7140839916909538 loss: 0.12885498851166965\n",
            "iteration 106: weight=0.7169698141815694 loss: 0.12675531621162062\n",
            "iteration 107: weight=0.719853148882477 loss: 0.12467407990103918\n",
            "iteration 108: weight=0.722733953138132 loss: 0.12261126669980804\n",
            "iteration 109: weight=0.7256121831691664 loss: 0.12056686356432833\n",
            "iteration 110: weight=0.7284877940394222 loss: 0.11854085728012331\n",
            "iteration 111: weight=0.7313607396212186 loss: 0.11653323445461994\n",
            "iteration 112: weight=0.7342309725588083 loss: 0.11454398151007172\n",
            "iteration 113: weight=0.7370984442299757 loss: 0.11257308467659177\n",
            "iteration 114: weight=0.7399631047057204 loss: 0.11062052998526717\n",
            "iteration 115: weight=0.7428249027079643 loss: 0.10868630326132389\n",
            "iteration 116: weight=0.7456837855652145 loss: 0.10677039011732115\n",
            "iteration 117: weight=0.7485396991661042 loss: 0.10487277594634814\n",
            "iteration 118: weight=0.7513925879107317 loss: 0.10299344591520232\n",
            "iteration 119: weight=0.7542423946597075 loss: 0.10113238495752885\n",
            "iteration 120: weight=0.7570890606808118 loss: 0.09928957776690095\n",
            "iteration 121: weight=0.75993252559316 loss: 0.09746500878982278\n",
            "iteration 122: weight=0.7627727273087629 loss: 0.0956586622186375\n",
            "iteration 123: weight=0.7656096019713624 loss: 0.09387052198432322\n",
            "iteration 124: weight=0.7684430838924142 loss: 0.09210057174916075\n",
            "iteration 125: weight=0.7712731054840793 loss: 0.09034879489925773\n",
            "iteration 126: weight=0.7740995971890783 loss: 0.08861517453691325\n",
            "iteration 127: weight=0.7769224874072511 loss: 0.0868996934728079\n",
            "iteration 128: weight=0.7797417024186563 loss: 0.08520233421800516\n",
            "iteration 129: weight=0.7825571663030296 loss: 0.08352307897574851\n",
            "iteration 130: weight=0.7853688008554148 loss: 0.08186190963304041\n",
            "iteration 131: weight=0.7881765254977618 loss: 0.08021880775198864\n",
            "iteration 132: weight=0.7909802571862794 loss: 0.07859375456090378\n",
            "iteration 133: weight=0.793779910314311 loss: 0.0769867309451353\n",
            "iteration 134: weight=0.796575396610492 loss: 0.07539771743762846\n",
            "iteration 135: weight=0.7993666250319258 loss: 0.07382669420919008\n",
            "iteration 136: weight=0.8021535016521044 loss: 0.07227364105844336\n",
            "iteration 137: weight=0.8049359295432782 loss: 0.07073853740145897\n",
            "iteration 138: weight=0.8077138086529599 loss: 0.0692213622610447\n",
            "iteration 139: weight=0.810487035674231 loss: 0.06772209425567532\n",
            "iteration 140: weight=0.8132555039094913 loss: 0.06624071158804701\n",
            "iteration 141: weight=0.8160191031272767 loss: 0.06477719203323629\n",
            "iteration 142: weight=0.8187777194117362 loss: 0.06333151292644557\n",
            "iteration 143: weight=0.8215312350043416 loss: 0.061903651150314376\n",
            "iteration 144: weight=0.824279528137368 loss: 0.06049358312177655\n",
            "iteration 145: weight=0.8270224728586575 loss: 0.0591012847784409\n",
            "iteration 146: weight=0.8297599388471416 loss: 0.05772673156447372\n",
            "iteration 147: weight=0.8324917912185692 loss: 0.056369898415958075\n",
            "iteration 148: weight=0.8352178903208431 loss: 0.055030759745706814\n",
            "iteration 149: weight=0.8379380915183339 loss: 0.053709289427502416\n",
            "iteration 150: weight=0.8406522449644935 loss: 0.052405460779735735\n",
            "iteration 151: weight=0.8433601953620496 loss: 0.05111924654841682\n",
            "iteration 152: weight=0.8460617817100089 loss: 0.04985061888952591\n",
            "iteration 153: weight=0.8487568370366502 loss: 0.048599549350674454\n",
            "iteration 154: weight=0.8514451881176307 loss: 0.047366008852041674\n",
            "iteration 155: weight=0.8541266551782698 loss: 0.04614996766655244\n",
            "iteration 156: weight=0.8568010515790138 loss: 0.044951395399259186\n",
            "iteration 157: weight=0.8594681834830169 loss: 0.04377026096588876\n",
            "iteration 158: weight=0.8621278495047044 loss: 0.04260653257051439\n",
            "iteration 159: weight=0.8647798403381065 loss: 0.04146017768230681\n",
            "iteration 160: weight=0.867423938363674 loss: 0.04033116301132289\n",
            "iteration 161: weight=0.8700599172322017 loss: 0.03921945448327879\n",
            "iteration 162: weight=0.8726875414243944 loss: 0.038125017213261336\n",
            "iteration 163: weight=0.8753065657845208 loss: 0.037047815478320256\n",
            "iteration 164: weight=0.8779167350264959 loss: 0.03598781268888651\n",
            "iteration 165: weight=0.8805177832106363 loss: 0.03494497135895558\n",
            "iteration 166: weight=0.8831094331892185 loss: 0.03391925307497268\n",
            "iteration 167: weight=0.8856913960188635 loss: 0.032910618463352415\n",
            "iteration 168: weight=0.8882633703376543 loss: 0.03191902715656203\n",
            "iteration 169: weight=0.8908250417047748 loss: 0.030944437757692067\n",
            "iteration 170: weight=0.8933760819003401 loss: 0.029986807803436544\n",
            "iteration 171: weight=0.8959161481829697 loss: 0.02904609372539635\n",
            "iteration 172: weight=0.8984448825025356 loss: 0.028122250809617833\n",
            "iteration 173: weight=0.9009619106654062 loss: 0.027215233154271545\n",
            "iteration 174: weight=0.9034668414493983 loss: 0.026324993625370996\n",
            "iteration 175: weight=0.9059592656655547 loss: 0.02545148381042517\n",
            "iteration 176: weight=0.9084387551637869 loss: 0.024594653969913005\n",
            "iteration 177: weight=0.910904861779359 loss: 0.023754452986460173\n",
            "iteration 178: weight=0.9133571162171659 loss: 0.022930828311591767\n",
            "iteration 179: weight=0.9157950268707606 loss: 0.022123725909927727\n",
            "iteration 180: weight=0.918218078573148 loss: 0.021333090200680638\n",
            "iteration 181: weight=0.9206257312764771 loss: 0.02055886399630258\n",
            "iteration 182: weight=0.9230174186579674 loss: 0.019800988438126277\n",
            "iteration 183: weight=0.9253925466496976 loss: 0.019059402928830016\n",
            "iteration 184: weight=0.9277504918903098 loss: 0.018334045061548676\n",
            "iteration 185: weight=0.9300906000972545 loss: 0.01762485054544405\n",
            "iteration 186: weight=0.9324121843589637 loss: 0.01693175312753392\n",
            "iteration 187: weight=0.9347145233473323 loss: 0.016254684510570883\n",
            "iteration 188: weight=0.9369968594521637 loss: 0.015593574266748345\n",
            "iteration 189: weight=0.9392583968408489 loss: 0.014948349746999323\n",
            "iteration 190: weight=0.9414982994485801 loss: 0.014318935985641601\n",
            "iteration 191: weight=0.9437156889069274 loss: 0.013705255600109267\n",
            "iteration 192: weight=0.9459096424217303 loss: 0.01310722868549663\n",
            "iteration 193: weight=0.9480791906150935 loss: 0.01252477270362895\n",
            "iteration 194: weight=0.9502233153509579 loss: 0.011957802366357795\n",
            "iteration 195: weight=0.9523409475693952 loss: 0.011406229512767596\n",
            "iteration 196: weight=0.954430965161622 loss: 0.010869962979966119\n",
            "iteration 197: weight=0.9564921909259337 loss: 0.010348908467116669\n",
            "iteration 198: weight=0.9585233906545294 loss: 0.009842968392360651\n",
            "iteration 199: weight=0.9605232714127514 loss: 0.009352041742264783\n",
            "iteration 200: weight=0.9624904800858315 loss: 0.008876023913420683\n",
            "iteration 201: weight=0.9644236022840156 loss: 0.008414806545814169\n",
            "iteration 202: weight=0.9663211617151355 loss: 0.00796827734758021\n",
            "iteration 203: weight=0.9681816201544251 loss: 0.007536319910754652\n",
            "iteration 204: weight=0.9700033781646732 loss: 0.007118813517641098\n",
            "iteration 205: weight=0.9717847767455491 loss: 0.006715632937417504\n",
            "iteration 206: weight=0.9735241001187812 loss: 0.006326648212625741\n",
            "iteration 207: weight=0.9752195798851573 loss: 0.005951724435211192\n",
            "iteration 208: weight=0.9768694008189638 loss: 0.005590721511817276\n",
            "iteration 209: weight=0.9784717085938408 loss: 0.005243493918087649\n",
            "iteration 210: weight=0.98002461975875 loss: 0.004909890441796461\n",
            "iteration 211: weight=0.9815262343006085 loss: 0.004589753914711392\n",
            "iteration 212: weight=0.9829746511369162 loss: 0.0042829209332040374\n",
            "iteration 213: weight=0.9843679868720687 loss: 0.003989221567759715\n",
            "iteration 214: weight=0.9857043981185931 loss: 0.0037084790617082364\n",
            "iteration 215: weight=0.9869821076218998 loss: 0.00344050951971131\n",
            "iteration 216: weight=0.9881994343264098 loss: 0.0031851215867990668\n",
            "iteration 217: weight=0.9893548273743451 loss: 0.0029421161190605893\n",
            "iteration 218: weight=0.990446903829584 loss: 0.00271128584747804\n",
            "iteration 219: weight=0.9914744896641454 loss: 0.002492415036842166\n",
            "iteration 220: weight=0.9924366632351936 loss: 0.0022852791422334207\n",
            "iteration 221: weight=0.9933328001240684 loss: 0.002089644466193974\n",
            "iteration 222: weight=0.994162617822982 loss: 0.0019052678204689588\n",
            "iteration 223: weight=0.9949262183676253 loss: 0.0017318961970791413\n",
            "iteration 224: weight=0.9956241266639864 loss: 0.0015692664545073898\n",
            "iteration 225: weight=0.9962573219936764 loss: 0.0014171050259546014\n",
            "iteration 226: weight=0.9968272600578566 loss: 0.0012751276579565651\n",
            "iteration 227: weight=0.9973358829879706 loss: 0.001143039189153261\n",
            "iteration 228: weight=0.9977856150541351 loss: 0.0010205333806634354\n",
            "iteration 229: weight=0.9981793423612345 loss: 0.0009072928113322831\n",
            "iteration 230: weight=0.9985203756315639 loss: 0.0008029888530541873\n",
            "iteration 231: weight=0.998812396189858 loss: 0.0007072817433737555\n",
            "iteration 232: weight=0.999059386415897 loss: 0.0006198207745765627\n",
            "iteration 233: weight=0.9992655471075379 loss: 0.0005402446203655042\n",
            "iteration 234: weight=0.9994352052828877 loss: 0.0004681818228421397\n",
            "iteration 235: weight=0.9995727168240934 loss: 0.0004032514636727047\n",
            "iteration 236: weight=0.9996823689228956 loss: 0.0003450640437523411\n",
            "iteration 237: weight=0.9997682874562497 loss: 0.00029322259508490767\n",
            "iteration 238: weight=0.9998343541656566 loss: 0.0002473240465943327\n",
            "iteration 239: weight=0.9998841378472391 loss: 0.00020696086178007\n",
            "iteration 240: weight=0.999920842735077 loss: 0.00017172296009249077\n",
            "iteration 241: weight=0.9999472759697656 loss: 0.00014119992525962743\n",
            "iteration 242: weight=0.9999658346088454 loss: 0.00011498349223959536\n",
            "iteration 243: weight=0.9999785111950743 loss: 9.26702899198828e-05\n",
            "iteration 244: weight=0.9999869155961294 loss: 7.386479929627487e-05\n",
            "iteration 245: weight=0.999992309797109 loss: 5.818246726696508e-05\n",
            "iteration 246: weight=0.9999956516682263 loss: 4.5252895434666485e-05\n",
            "iteration 247: weight=0.9999976435029695 loss: 3.472300312545862e-05\n",
            "iteration 248: weight=0.9999987813300462 loss: 2.6260046386283875e-05\n",
            "iteration 249: weight=0.9999994015906678 loss: 1.9554362665430425e-05\n",
            "iteration 250: weight=0.9999997226351449 loss: 1.432170695092605e-05\n",
            "iteration 251: weight=0.9999998794896566 loss: 1.0305051806325238e-05\n",
            "iteration 252: weight=0.9999999513267326 loss: 7.275742637147253e-06\n",
            "iteration 253: weight=0.9999999819106817 loss: 5.0339308924618464e-06\n",
            "iteration 254: weight=0.9999999938919832 loss: 3.4082502299304096e-06\n",
            "iteration 255: weight=0.9999999981564109 loss: 2.254750428698493e-06\n",
            "iteration 256: weight=0.9999999995131721 loss: 1.4551558268349751e-06\n",
            "iteration 257: weight=0.9999999998908256 loss: 9.145630048834619e-07\n",
            "iteration 258: weight=0.999999999980098 loss: 5.587300796472405e-07\n",
            "iteration 259: weight=0.9999999999972523 loss: 3.311321895571595e-07\n",
            "iteration 260: weight=0.9999999999997488 loss: 1.8996167971963018e-07\n",
            "iteration 261: weight=0.9999999999999893 loss: 1.0523711135856928e-07\n",
            "iteration 262: weight=1.0 loss: 5.615540765724347e-08\n",
            "converged after 263 iterations\n"
          ]
        }
      ],
      "source": [
        "w_0 = random.random()\n",
        "b_0 = random.random()\n",
        "rmsprop= RMSProp()\n",
        "\n",
        "t = 1\n",
        "converged = False\n",
        "iter_rms, loss_rms= [], []\n",
        "\n",
        "while not converged:\n",
        "    dw = grad_function(w_0)\n",
        "    db = grad_function(b_0)\n",
        "    w_0_old = w_0\n",
        "    w_0, b_0, loss = rmsprop.update(t,w=w_0, b=b_0, dw=dw, db=db)\n",
        "    if check_convergence(w_0, w_0_old):\n",
        "        print('converged after '+str(t)+' iterations')\n",
        "        break\n",
        "    else:\n",
        "      iter_rms.append(t)\n",
        "      loss_rms.append((loss))\n",
        "      print('iteration '+str(t)+': weight='+str(w_0)+\" loss: \"+str(loss))\n",
        "      t+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "8iL8n3OpyDTx",
        "outputId": "06d30f5d-de70-4728-9d5a-f282031a6fcf"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#plt.plot(iter, loss, label= \"Adam\")\n",
        "plt.plot(iter_rms, loss_rms, label= \"RMSProp\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021, 13:44:55) [MSC v.1928 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "f19fa5a75a2a91d5c33c6e6f47fe7e0790f874506265538e59819ef6a8fb5bd4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
